{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data manipulation\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Pytorch geometric\n",
    "import torch   \n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GIN\n",
    "from torch.nn import Linear, CrossEntropyLoss, BCELoss\n",
    "import torch_geometric.transforms as T\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.nn import global_mean_pool as gap,  global_max_pool as gmp, global_add_pool as gsp\n",
    "\n",
    "#rdkit\n",
    "from rdkit import Chem                      \n",
    "from rdkit.Chem import GetAdjacencyMatrix       \n",
    "from scipy.sparse import coo_matrix\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit import Chem, DataStructs\n",
    "\n",
    "#matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#chemprop\n",
    "import chemprop\n",
    "from chemprop.args import TrainArgs, PredictArgs\n",
    "from chemprop.train import cross_validate, run_training, make_predictions\n",
    "\n",
    "#sklearn\n",
    "import sklearn\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "#shuffle\n",
    "from random import shuffle\n",
    "\n",
    "#GPU\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onek_encoding_unk(value, choices):\n",
    "    \"\"\"\n",
    "    Creates a one-hot encoding with an extra category for uncommon values.\n",
    "\n",
    "    :param value: The value for which the encoding should be one.\n",
    "    :param choices: A list of possible values.\n",
    "    :return: A one-hot encoding of the :code:`value` in a list of length :code:`len(choices) + 1`.\n",
    "             If :code:`value` is not in :code:`choices`, then the final element in the encoding is -1.\n",
    "    \"\"\"\n",
    "    encoding = [0] * (len(choices) + 1)\n",
    "    index = choices.index(value) if value in choices else -1\n",
    "    encoding[index] = 1\n",
    "\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Featurization_parameters:\n",
    "    \"\"\"\n",
    "    A class holding molecule featurization parameters as attributes.\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "\n",
    "        # Atom feature sizes\n",
    "        self.MAX_ATOMIC_NUM = 100\n",
    "        #for one-hot-encoding\n",
    "        self.ATOM_FEATURES = {\n",
    "            'atomic_num': list(range(self.MAX_ATOMIC_NUM)),\n",
    "            'degree': [0, 1, 2, 3, 4, 5],\n",
    "            'formal_charge': [-1, -2, 1, 2, 0],\n",
    "            'chiral_tag': [0, 1, 2, 3],\n",
    "            'num_Hs': [0, 1, 2, 3, 4],\n",
    "            'hybridization': [\n",
    "                Chem.rdchem.HybridizationType.SP,\n",
    "                Chem.rdchem.HybridizationType.SP2,\n",
    "                Chem.rdchem.HybridizationType.SP3,\n",
    "                Chem.rdchem.HybridizationType.SP3D,\n",
    "                Chem.rdchem.HybridizationType.SP3D2\n",
    "            ],\n",
    "        }\n",
    "\n",
    "        # Distance feature sizes\n",
    "        self.PATH_DISTANCE_BINS = list(range(10))\n",
    "        self.THREE_D_DISTANCE_MAX = 20\n",
    "        self.THREE_D_DISTANCE_STEP = 1\n",
    "        self.THREE_D_DISTANCE_BINS = list(range(0, self.THREE_D_DISTANCE_MAX + 1, self.THREE_D_DISTANCE_STEP))\n",
    "\n",
    "        # len(choices) + 1 to include room for uncommon values; + 2 at end for IsAromatic and mass\n",
    "        self.ATOM_FDIM = sum(len(choices) + 1 for choices in self.ATOM_FEATURES.values()) + 2\n",
    "        self.EXTRA_ATOM_FDIM = 0\n",
    "        self.BOND_FDIM = 14\n",
    "        self.EXTRA_BOND_FDIM = 0\n",
    "        self.REACTION_MODE = None\n",
    "        self.EXPLICIT_H = False\n",
    "        self.REACTION = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = Featurization_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atom_features(atom: Chem.rdchem.Atom, functional_groups=None):\n",
    "    \"\"\"\n",
    "    Builds a feature vector for an atom.\n",
    "\n",
    "    :param atom: An RDKit atom.\n",
    "    :param functional_groups: A k-hot vector indicating the functional groups the atom belongs to.\n",
    "    :return: A list containing the atom features.\n",
    "    \"\"\"\n",
    "    if atom is None:\n",
    "        features = [0] * PARAMS.ATOM_FDIM\n",
    "    else:\n",
    "        features = onek_encoding_unk(atom.GetAtomicNum() - 1, PARAMS.ATOM_FEATURES['atomic_num']) + \\\n",
    "            onek_encoding_unk(atom.GetTotalDegree(), PARAMS.ATOM_FEATURES['degree']) + \\\n",
    "            onek_encoding_unk(atom.GetFormalCharge(), PARAMS.ATOM_FEATURES['formal_charge']) + \\\n",
    "            onek_encoding_unk(int(atom.GetChiralTag()), PARAMS.ATOM_FEATURES['chiral_tag']) + \\\n",
    "            onek_encoding_unk(int(atom.GetTotalNumHs()), PARAMS.ATOM_FEATURES['num_Hs']) + \\\n",
    "            onek_encoding_unk(int(atom.GetHybridization()), PARAMS.ATOM_FEATURES['hybridization']) + \\\n",
    "            [1 if atom.GetIsAromatic() else 0] + \\\n",
    "            [atom.GetMass() * 0.01]  # scaled to about the same range as other features\n",
    "        if functional_groups is not None:\n",
    "            features += functional_groups\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bond_features(bond: Chem.rdchem.Bond):\n",
    "    \"\"\"\n",
    "    Builds a feature vector for a bond.\n",
    "\n",
    "    :param bond: An RDKit bond.\n",
    "    :return: A list containing the bond features.\n",
    "    \"\"\"\n",
    "    if bond is None:\n",
    "        fbond = [1] + [0] * (PARAMS.BOND_FDIM - 1)\n",
    "    else:\n",
    "        bt = bond.GetBondType()\n",
    "        fbond = [\n",
    "            0,  # bond is not None\n",
    "            bt == Chem.rdchem.BondType.SINGLE,\n",
    "            bt == Chem.rdchem.BondType.DOUBLE,\n",
    "            bt == Chem.rdchem.BondType.TRIPLE,\n",
    "            bt == Chem.rdchem.BondType.AROMATIC,\n",
    "            (bond.GetIsConjugated() if bt is not None else 0),\n",
    "            (bond.IsInRing() if bt is not None else 0)\n",
    "        ]\n",
    "        fbond += onek_encoding_unk(int(bond.GetStereo()), list(range(6)))\n",
    "    return fbond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MORGAN_RADIUS = 2\n",
    "MORGAN_NUM_BITS = 2048\n",
    "#a vector representation (1x2048) for molecular feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def morgan_binary_features_generator(mol,\n",
    "                                     radius: int = MORGAN_RADIUS,\n",
    "                                     num_bits: int = MORGAN_NUM_BITS):\n",
    "    \"\"\"\n",
    "    Generates a binary Morgan fingerprint for a molecule.\n",
    "    :param mol: A molecule (i.e., either a SMILES or an RDKit molecule).\n",
    "    :param radius: Morgan fingerprint radius.\n",
    "    :param num_bits: Number of bits in Morgan fingerprint.\n",
    "    :return: A 1D numpy array containing the binary Morgan fingerprint.\n",
    "    \"\"\"\n",
    "    mol = Chem.MolFromSmiles(mol) if type(mol) == str else mol\n",
    "    features_vec = AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=num_bits)\n",
    "    features = np.zeros((1,))\n",
    "    DataStructs.ConvertToNumpyArray(features_vec, features)\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(dataset,batch_size):\n",
    "    SMILES = dataset['SMILES']\n",
    "    data_list = []\n",
    "    for smiles in SMILES:\n",
    "        mol = Chem.MolFromSmiles(smiles)     \n",
    "        mol = Chem.AddHs(mol)  \n",
    "\n",
    "        #generate a global vector features (binary Morgan fingerprint) and convert them\n",
    "        mol_feature = torch.tensor(np.array(morgan_binary_features_generator(mol)))\n",
    "\n",
    "        xs = []\n",
    "        for atom in mol.GetAtoms():\n",
    "            x = atom_features(atom)\n",
    "            xs.append(x)\n",
    "            \n",
    "        x = torch.tensor(np.array(xs))\n",
    "        \n",
    "        edge_indices, edge_attrs = [], []\n",
    "        \n",
    "        for bond in mol.GetBonds():\n",
    "            i = bond.GetBeginAtomIdx()\n",
    "            j = bond.GetEndAtomIdx()\n",
    "    \n",
    "            e = bond_features(bond)\n",
    "\n",
    "            edge_indices += [[i,j],[j,i]]\n",
    "            edge_attrs += [e, e]\n",
    "        \n",
    "        edge_index = torch.tensor(edge_indices)\n",
    "        edge_index = edge_index.t().to(torch.long).view(2, -1)\n",
    "        edge_attr = torch.tensor(edge_attrs).view(-1, 14)\n",
    "        \n",
    "        y = torch.tensor(int(dataset.loc[dataset['SMILES'] == smiles,'Activity'])) #response variable y\n",
    "\n",
    "        smi = smiles\n",
    "\n",
    "        # add smiles and num_feature as the attributes\n",
    "        data = Data(x=x, y=y, edge_index=edge_index,edge_attr = edge_attr, smiles=smi, mol_feature=mol_feature)  \n",
    "        data_list.append(data)   # store processed data into the list\n",
    "        \n",
    "    return DataLoader(data_list,batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print            \n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_weights(m):\n",
    "  '''\n",
    "    Try resetting model weights to avoid\n",
    "    weight leakage.\n",
    "  '''\n",
    "  for layer in m.children():\n",
    "    if hasattr(layer, 'reset_parameters'):\n",
    "        print(f'Reset trainable parameters of layer = {layer}')\n",
    "        layer.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch,train_loader):\n",
    "    \n",
    "    model.train()   \n",
    "    running_loss = 0 \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    criterion = BCELoss()\n",
    "    for batch in train_loader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch)\n",
    "        label = batch.y.view(-1,1)\n",
    "        loss = criterion(outputs.float(),label.float())\n",
    "        \n",
    "\n",
    "        loss.backward()   # Compute the gradient of loss function \n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        running_loss += loss.item()\n",
    "        # probability that is larger than 0.5, classify as 1 \n",
    "\n",
    "        pred = (outputs >= 0.5).float()\n",
    "\n",
    "        total += label.size(0)\n",
    "        correct += (pred == label).float().sum()\n",
    "        \n",
    "    \n",
    "    loss = running_loss/len(train_loader)\n",
    "    accuracy = 100*correct/total\n",
    "    \n",
    "    train_accuracy.append(accuracy)\n",
    "    train_loss.append(loss)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch: '+str(int(epoch)))\n",
    "        print('Train Loss: %.3f | Accuracy: %.3f'%(loss,accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch,test_loader):\n",
    "    model.eval()\n",
    "    \n",
    "    running_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        criterion = BCELoss()\n",
    "        for batch in test_loader:\n",
    "        \n",
    "            outputs = model(batch)\n",
    "            label = batch.y.view(-1,1)\n",
    "\n",
    "            loss = criterion(outputs.float(), label.float())    \n",
    "            running_loss += loss.item()\n",
    "            # probability that is larger than 0.5, classify as 1 \n",
    "            pred = (outputs >= 0.5).float()\n",
    "\n",
    "            total += label.size(0)\n",
    "            correct += (pred == label).float().sum()\n",
    "    \n",
    "        loss = running_loss/len(test_loader)\n",
    "        accuracy = 100*correct/total\n",
    "    \n",
    "        test_accuracy.append(accuracy)\n",
    "        test_loss.append(loss)\n",
    "        if epoch % 10 == 0:\n",
    "            print('Test Loss: %.3f | Accuracy: %.3f'%(loss,accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_set as a whole loader\n",
    "def test_metrics(test_loader):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        labels = []\n",
    "        preds = []\n",
    "        for batch in test_loader:\n",
    "            \n",
    "            labels += list(batch.y.view(-1,1).numpy())\n",
    "            preds += list(model(batch).detach().numpy())\n",
    "        \n",
    "        pred_labels = [1 if i > 0.5 else 0 for i in preds]\n",
    "        auc = roc_auc_score(list(labels), list(preds), average='weighted')\n",
    "        report = classification_report(labels, pred_labels,output_dict=True)\n",
    "        return auc, report\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(metrics):\n",
    "    AUC = [] #0\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1_score = []\n",
    "    accuracy = []\n",
    "    for i in metrics:\n",
    "        AUC.append(i[0])\n",
    "        precision.append(i[1]['weighted avg']['precision'])\n",
    "        recall.append(i[1]['weighted avg']['recall'])\n",
    "        f1_score.append(i[1]['weighted avg']['f1-score'])\n",
    "        accuracy.append(i[1]['accuracy'])\n",
    "    \n",
    "    print('AUC:',np.mean(AUC),'+/-',np.std(AUC))\n",
    "    print('Accuracy:',np.mean(accuracy),'+/-',np.std(accuracy))\n",
    "    print('Precision:',np.mean(precision),'+/-',np.std(precision))\n",
    "    print('Recall:',np.mean(recall),'+/-',np.std(recall))\n",
    "    print('F1-score:',np.mean(f1_score),'+/-',np.std(f1_score))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model construction for GIN\n",
    "\n",
    "#### 1. GIN layer to update node(atom) feature vector of a graph(modelcue).\n",
    "#### 2. Aggregate the updated node feature vector to capture global property\n",
    "####     i.e. apply global_add_pool function over the node features \n",
    "#### 3. Then pass the processed features to a fully connected layer for binary classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GIN_graph(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GIN_graph, self).__init__()\n",
    "        \n",
    "        self.conv1 = GIN(in_channels = 133, \n",
    "                         hidden_channels = 50, \n",
    "                         num_layers = 5)\n",
    "        \n",
    "        self.linear1 = Linear(50, 10)\n",
    "        self.linear2 = Linear(10, 1)\n",
    "        \n",
    "    def forward(self,data):\n",
    "        x, edge_index, batch_index, mol_feature = data.x, data.edge_index, data.batch, data.mol_feature\n",
    "        x = self.conv1(x,edge_index)\n",
    "        \n",
    "        x = gsp(x,batch_index)\n",
    "        \n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = torch.sigmoid(self.linear2(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GIN model training and evalutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1 ......\n",
      "Reset trainable parameters of layer = Linear(133, 50, bias=True)\n",
      "Reset trainable parameters of layer = Linear(50, 50, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d((50,), eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = MLP(133, 50, 50)\n",
      "Reset trainable parameters of layer = Linear(50, 50, bias=True)\n",
      "Reset trainable parameters of layer = Linear(50, 50, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d((50,), eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = MLP(50, 50, 50)\n",
      "Reset trainable parameters of layer = Linear(50, 50, bias=True)\n",
      "Reset trainable parameters of layer = Linear(50, 50, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d((50,), eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = MLP(50, 50, 50)\n",
      "Reset trainable parameters of layer = Linear(50, 50, bias=True)\n",
      "Reset trainable parameters of layer = Linear(50, 50, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d((50,), eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = MLP(50, 50, 50)\n",
      "Reset trainable parameters of layer = Linear(50, 50, bias=True)\n",
      "Reset trainable parameters of layer = Linear(50, 50, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d((50,), eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = MLP(50, 50, 50)\n",
      "Reset trainable parameters of layer = GINConv(nn=MLP(133, 50, 50))\n",
      "Reset trainable parameters of layer = GINConv(nn=MLP(50, 50, 50))\n",
      "Reset trainable parameters of layer = GINConv(nn=MLP(50, 50, 50))\n",
      "Reset trainable parameters of layer = GINConv(nn=MLP(50, 50, 50))\n",
      "Reset trainable parameters of layer = GINConv(nn=MLP(50, 50, 50))\n",
      "Reset trainable parameters of layer = GIN(133, 50, num_layers=5)\n",
      "Reset trainable parameters of layer = Linear(in_features=50, out_features=10, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=10, out_features=1, bias=True)\n",
      "Epoch: 0\n",
      "Train Loss: 0.565 | Accuracy: 78.646\n",
      "Test Loss: 0.788 | Accuracy: 83.333\n",
      "Validation loss decreased (inf --> 0.787531).  Saving model ...\n",
      "Validation loss decreased (0.787531 --> 0.506726).  Saving model ...\n",
      "Validation loss decreased (0.506726 --> 0.347700).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Validation loss decreased (0.347700 --> 0.346999).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Epoch: 10\n",
      "Train Loss: 0.245 | Accuracy: 92.188\n",
      "Test Loss: 0.528 | Accuracy: 78.472\n",
      "EarlyStopping counter: 5 out of 10\n",
      "EarlyStopping counter: 6 out of 10\n",
      "EarlyStopping counter: 7 out of 10\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Validation loss decreased (0.346999 --> 0.319401).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Validation loss decreased (0.319401 --> 0.315023).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch: 20\n",
      "Train Loss: 0.150 | Accuracy: 94.618\n",
      "Test Loss: 0.432 | Accuracy: 85.417\n",
      "EarlyStopping counter: 3 out of 10\n",
      "EarlyStopping counter: 4 out of 10\n",
      "EarlyStopping counter: 5 out of 10\n",
      "EarlyStopping counter: 6 out of 10\n",
      "EarlyStopping counter: 7 out of 10\n",
      "EarlyStopping counter: 8 out of 10\n",
      "EarlyStopping counter: 9 out of 10\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "Split 2 ......\n",
      "Reset trainable parameters of layer = Linear(133, 50, bias=True)\n",
      "Reset trainable parameters of layer = Linear(50, 50, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d((50,), eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = MLP(133, 50, 50)\n",
      "Reset trainable parameters of layer = Linear(50, 50, bias=True)\n",
      "Reset trainable parameters of layer = Linear(50, 50, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d((50,), eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = MLP(50, 50, 50)\n",
      "Reset trainable parameters of layer = Linear(50, 50, bias=True)\n",
      "Reset trainable parameters of layer = Linear(50, 50, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d((50,), eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = MLP(50, 50, 50)\n",
      "Reset trainable parameters of layer = Linear(50, 50, bias=True)\n",
      "Reset trainable parameters of layer = Linear(50, 50, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d((50,), eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = MLP(50, 50, 50)\n",
      "Reset trainable parameters of layer = Linear(50, 50, bias=True)\n",
      "Reset trainable parameters of layer = Linear(50, 50, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d((50,), eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = MLP(50, 50, 50)\n",
      "Reset trainable parameters of layer = GINConv(nn=MLP(133, 50, 50))\n",
      "Reset trainable parameters of layer = GINConv(nn=MLP(50, 50, 50))\n",
      "Reset trainable parameters of layer = GINConv(nn=MLP(50, 50, 50))\n",
      "Reset trainable parameters of layer = GINConv(nn=MLP(50, 50, 50))\n",
      "Reset trainable parameters of layer = GINConv(nn=MLP(50, 50, 50))\n",
      "Reset trainable parameters of layer = GIN(133, 50, num_layers=5)\n",
      "Reset trainable parameters of layer = Linear(in_features=50, out_features=10, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=10, out_features=1, bias=True)\n",
      "Epoch: 0\n",
      "Train Loss: 0.534 | Accuracy: 79.688\n",
      "Test Loss: 0.503 | Accuracy: 83.333\n",
      "Validation loss decreased (inf --> 0.502893).  Saving model ...\n",
      "Validation loss decreased (0.502893 --> 0.446516).  Saving model ...\n",
      "Validation loss decreased (0.446516 --> 0.414307).  Saving model ...\n",
      "Validation loss decreased (0.414307 --> 0.345055).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.345055 --> 0.322873).  Saving model ...\n",
      "Validation loss decreased (0.322873 --> 0.298404).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Epoch: 10\n",
      "Train Loss: 0.261 | Accuracy: 89.236\n",
      "Test Loss: 0.325 | Accuracy: 85.417\n",
      "EarlyStopping counter: 4 out of 10\n",
      "EarlyStopping counter: 5 out of 10\n",
      "EarlyStopping counter: 6 out of 10\n",
      "EarlyStopping counter: 7 out of 10\n",
      "EarlyStopping counter: 8 out of 10\n",
      "EarlyStopping counter: 9 out of 10\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "Split 3 ......\n",
      "Reset trainable parameters of layer = Linear(133, 50, bias=True)\n",
      "Reset trainable parameters of layer = Linear(50, 50, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d((50,), eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = MLP(133, 50, 50)\n",
      "Reset trainable parameters of layer = Linear(50, 50, bias=True)\n",
      "Reset trainable parameters of layer = Linear(50, 50, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d((50,), eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = MLP(50, 50, 50)\n",
      "Reset trainable parameters of layer = Linear(50, 50, bias=True)\n",
      "Reset trainable parameters of layer = Linear(50, 50, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d((50,), eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = MLP(50, 50, 50)\n",
      "Reset trainable parameters of layer = Linear(50, 50, bias=True)\n",
      "Reset trainable parameters of layer = Linear(50, 50, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d((50,), eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = MLP(50, 50, 50)\n",
      "Reset trainable parameters of layer = Linear(50, 50, bias=True)\n",
      "Reset trainable parameters of layer = Linear(50, 50, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d((50,), eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = MLP(50, 50, 50)\n",
      "Reset trainable parameters of layer = GINConv(nn=MLP(133, 50, 50))\n",
      "Reset trainable parameters of layer = GINConv(nn=MLP(50, 50, 50))\n",
      "Reset trainable parameters of layer = GINConv(nn=MLP(50, 50, 50))\n",
      "Reset trainable parameters of layer = GINConv(nn=MLP(50, 50, 50))\n",
      "Reset trainable parameters of layer = GINConv(nn=MLP(50, 50, 50))\n",
      "Reset trainable parameters of layer = GIN(133, 50, num_layers=5)\n",
      "Reset trainable parameters of layer = Linear(in_features=50, out_features=10, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=10, out_features=1, bias=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Train Loss: 0.564 | Accuracy: 75.347\n",
      "Test Loss: 0.550 | Accuracy: 82.639\n",
      "Validation loss decreased (inf --> 0.549930).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.549930 --> 0.486053).  Saving model ...\n",
      "Validation loss decreased (0.486053 --> 0.460039).  Saving model ...\n",
      "Validation loss decreased (0.460039 --> 0.417751).  Saving model ...\n",
      "Validation loss decreased (0.417751 --> 0.408729).  Saving model ...\n",
      "Validation loss decreased (0.408729 --> 0.346424).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Epoch: 10\n",
      "Train Loss: 0.256 | Accuracy: 90.278\n",
      "Test Loss: 0.428 | Accuracy: 87.500\n",
      "EarlyStopping counter: 4 out of 10\n",
      "EarlyStopping counter: 5 out of 10\n",
      "EarlyStopping counter: 6 out of 10\n",
      "EarlyStopping counter: 7 out of 10\n",
      "EarlyStopping counter: 8 out of 10\n",
      "EarlyStopping counter: 9 out of 10\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "Split 4 ......\n",
      "Reset trainable parameters of layer = Linear(133, 50, bias=True)\n",
      "Reset trainable parameters of layer = Linear(50, 50, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d((50,), eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = MLP(133, 50, 50)\n",
      "Reset trainable parameters of layer = Linear(50, 50, bias=True)\n",
      "Reset trainable parameters of layer = Linear(50, 50, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d((50,), eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = MLP(50, 50, 50)\n",
      "Reset trainable parameters of layer = Linear(50, 50, bias=True)\n",
      "Reset trainable parameters of layer = Linear(50, 50, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d((50,), eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = MLP(50, 50, 50)\n",
      "Reset trainable parameters of layer = Linear(50, 50, bias=True)\n",
      "Reset trainable parameters of layer = Linear(50, 50, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d((50,), eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = MLP(50, 50, 50)\n",
      "Reset trainable parameters of layer = Linear(50, 50, bias=True)\n",
      "Reset trainable parameters of layer = Linear(50, 50, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d((50,), eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = MLP(50, 50, 50)\n",
      "Reset trainable parameters of layer = GINConv(nn=MLP(133, 50, 50))\n",
      "Reset trainable parameters of layer = GINConv(nn=MLP(50, 50, 50))\n",
      "Reset trainable parameters of layer = GINConv(nn=MLP(50, 50, 50))\n",
      "Reset trainable parameters of layer = GINConv(nn=MLP(50, 50, 50))\n",
      "Reset trainable parameters of layer = GINConv(nn=MLP(50, 50, 50))\n",
      "Reset trainable parameters of layer = GIN(133, 50, num_layers=5)\n",
      "Reset trainable parameters of layer = Linear(in_features=50, out_features=10, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=10, out_features=1, bias=True)\n",
      "Epoch: 0\n",
      "Train Loss: 0.490 | Accuracy: 81.424\n",
      "Test Loss: 0.544 | Accuracy: 83.333\n",
      "Validation loss decreased (inf --> 0.544130).  Saving model ...\n",
      "Validation loss decreased (0.544130 --> 0.411427).  Saving model ...\n",
      "Validation loss decreased (0.411427 --> 0.340831).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Validation loss decreased (0.340831 --> 0.291742).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Validation loss decreased (0.291742 --> 0.268606).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch: 10\n",
      "Train Loss: 0.229 | Accuracy: 92.708\n",
      "Test Loss: 0.373 | Accuracy: 88.889\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "EarlyStopping counter: 4 out of 10\n",
      "EarlyStopping counter: 5 out of 10\n",
      "EarlyStopping counter: 6 out of 10\n",
      "EarlyStopping counter: 7 out of 10\n",
      "EarlyStopping counter: 8 out of 10\n",
      "EarlyStopping counter: 9 out of 10\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "Split 5 ......\n",
      "Reset trainable parameters of layer = Linear(133, 50, bias=True)\n",
      "Reset trainable parameters of layer = Linear(50, 50, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d((50,), eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = MLP(133, 50, 50)\n",
      "Reset trainable parameters of layer = Linear(50, 50, bias=True)\n",
      "Reset trainable parameters of layer = Linear(50, 50, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d((50,), eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = MLP(50, 50, 50)\n",
      "Reset trainable parameters of layer = Linear(50, 50, bias=True)\n",
      "Reset trainable parameters of layer = Linear(50, 50, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d((50,), eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = MLP(50, 50, 50)\n",
      "Reset trainable parameters of layer = Linear(50, 50, bias=True)\n",
      "Reset trainable parameters of layer = Linear(50, 50, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d((50,), eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = MLP(50, 50, 50)\n",
      "Reset trainable parameters of layer = Linear(50, 50, bias=True)\n",
      "Reset trainable parameters of layer = Linear(50, 50, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d((50,), eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = MLP(50, 50, 50)\n",
      "Reset trainable parameters of layer = GINConv(nn=MLP(133, 50, 50))\n",
      "Reset trainable parameters of layer = GINConv(nn=MLP(50, 50, 50))\n",
      "Reset trainable parameters of layer = GINConv(nn=MLP(50, 50, 50))\n",
      "Reset trainable parameters of layer = GINConv(nn=MLP(50, 50, 50))\n",
      "Reset trainable parameters of layer = GINConv(nn=MLP(50, 50, 50))\n",
      "Reset trainable parameters of layer = GIN(133, 50, num_layers=5)\n",
      "Reset trainable parameters of layer = Linear(in_features=50, out_features=10, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=10, out_features=1, bias=True)\n",
      "Epoch: 0\n",
      "Train Loss: 0.608 | Accuracy: 65.104\n",
      "Test Loss: 0.547 | Accuracy: 83.333\n",
      "Validation loss decreased (inf --> 0.547190).  Saving model ...\n",
      "Validation loss decreased (0.547190 --> 0.471844).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Validation loss decreased (0.471844 --> 0.466232).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Validation loss decreased (0.466232 --> 0.427513).  Saving model ...\n",
      "Epoch: 10\n",
      "Train Loss: 0.293 | Accuracy: 89.410\n",
      "Test Loss: 0.409 | Accuracy: 85.417\n",
      "Validation loss decreased (0.427513 --> 0.409435).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "EarlyStopping counter: 4 out of 10\n",
      "EarlyStopping counter: 5 out of 10\n",
      "EarlyStopping counter: 6 out of 10\n",
      "EarlyStopping counter: 7 out of 10\n",
      "EarlyStopping counter: 8 out of 10\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Epoch: 20\n",
      "Train Loss: 0.304 | Accuracy: 87.153\n",
      "Test Loss: 0.404 | Accuracy: 73.611\n",
      "Validation loss decreased (0.409435 --> 0.404493).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "EarlyStopping counter: 4 out of 10\n",
      "EarlyStopping counter: 5 out of 10\n",
      "EarlyStopping counter: 6 out of 10\n",
      "EarlyStopping counter: 7 out of 10\n",
      "EarlyStopping counter: 8 out of 10\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Epoch: 30\n",
      "Train Loss: 0.255 | Accuracy: 89.410\n",
      "Test Loss: 0.454 | Accuracy: 81.944\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "test_losses= []\n",
    "metrics = []\n",
    "\n",
    "for i in range(5):\n",
    "    \n",
    "    print('Split '+str(i+1)+' ......')\n",
    "    train_set = pd.read_csv('C:/Users/jimmy/Desktop/FYP/train_split'+str(i+1)+'.csv')\n",
    "    test_set = pd.read_csv('C:/Users/jimmy/Desktop/FYP/test_split'+str(i+1)+'.csv')\n",
    "    train_loader = data_process(train_set,32)\n",
    "    test_loader = data_process(test_set,len(test_set))\n",
    "    \n",
    "    model = GIN_graph().double()\n",
    "    model.apply(reset_weights)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001,weight_decay=0.001)\n",
    "    \n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    train_accuracy = []\n",
    "    test_accuracy = []\n",
    "    \n",
    "    early_stopping = EarlyStopping(patience=10, verbose=True)\n",
    "    #update the model parameters with 50 epochs\n",
    "    for epoch in range(100):\n",
    "        train(epoch,train_loader)\n",
    "        test(epoch,test_loader)\n",
    "        \n",
    "        early_stopping(test_loss[-1],model)\n",
    "        \n",
    "        if early_stopping.early_stop:\n",
    "            print('Early stopping')\n",
    "            break\n",
    "    \n",
    "    model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "    auc, report = test_metrics(test_loader)\n",
    "    metric = [auc,report]\n",
    "    \n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    train_acc.append(train_accuracy)\n",
    "    metrics.append(metric)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.8443055555555556 +/- 0.04673079670956948\n",
      "Accuracy: 0.8486111111111111 +/- 0.056314263701981344\n",
      "Precision: 0.8626555826924036 +/- 0.020899459861554903\n",
      "Recall: 0.8486111111111111 +/- 0.056314263701981344\n",
      "F1-score: 0.8514049773042174 +/- 0.043983882496476646\n"
     ]
    }
   ],
   "source": [
    "print_metrics(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
